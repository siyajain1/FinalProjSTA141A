---
title: "Final Project"
author: "Siya Jain"
date: "2024-03-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(readr)
library(tidyverse)
library(caret) 

suppressWarnings(library(tidyverse))
suppressWarnings(library(knitr))
suppressWarnings(library(dplyr))

```

```{r}
session=list()
for(i in 1:18){
  session[[i]]=readRDS(paste('./Data/session',i,'.rds',sep=''))
}
```

```{r}
n.session=length(session)

# in library tidyverse
meta <- tibble(
  mouse_name = rep('name',n.session),
  date_exp =rep('dt',n.session),
  n_brain_area = rep(0,n.session),
  n_neurons = rep(0,n.session),
  n_trials = rep(0,n.session),
  success_rate = rep(0,n.session)
)


for(i in 1:n.session){
  tmp = session[[i]];
  meta[i,1]=tmp$mouse_name;
  meta[i,2]=tmp$date_exp;
  meta[i,3]=length(unique(tmp$brain_area));
  meta[i,4]=dim(tmp$spks[[1]])[1];
  meta[i,5]=length(tmp$feedback_type);
  meta[i,6]=mean(tmp$feedback_type+1)/2;
}

kable(meta, format = "html", table.attr = "class='table table-striped'",digits=2) 
```

```{r}
x<- c()
for (i in 1:18){
  x[i] = length(session[[i]]$brain_area)
}

tibble("session_id" = c(1:18), 'total_neurons' = x)

num_ba<- c()
for (i in c(1:18)){
  num_ba[i] = length(unique(session[[i]]$brain_area))
}
tibble("session_id" = c(1:18), 'number_of_brain_areas' = num_ba)
```


```{r}
get_session_data <- function(session_id) {
  # Initialize an empty list to store data for each trial
  trial_data_list <- list()
  
  # Iterate over all trails in the session
  for (trail_id in seq_along(session[[session_id]]$spks)) {
    spikes <- session[[session_id]]$spks[[trail_id]]
    
    # Check for missing values
    if (any(is.na(spikes))) {
      message("Missing value in session ", session_id, ", trail ", trail_id)
      next  # Skip to the next iteration if there are missing values
    }
    
    # Calculate neuron spike sums
    neuron_spike_sum <- rowSums(spikes)
    
    # Create a tibble with neuron_spike, brain_area, and calculate region_sum_spike, region_count, and region_mean_spike
    trail_tibble <- tibble(
      neuron_spike = neuron_spike_sum,
      brain_area = session[[session_id]]$brain_area
    ) %>% 
      group_by(brain_area) %>% 
      summarize(
        region_sum_spike = sum(neuron_spike),
        region_count = n(),
        region_mean_spike = mean(neuron_spike)
      ) 
    
    # Add columns for additional trail information
    trail_tibble <- trail_tibble %>% 
      add_column(
        trail_id = trail_id,
        contrast_left = session[[session_id]]$contrast_left[trail_id],
        contrast_right = session[[session_id]]$contrast_right[trail_id],
        feedback_type = session[[session_id]]$feedback_type[trail_id],
        contrast_diff = 
abs(session[[session_id]]$contrast_left[trail_id] - session[[session_id]]$contrast_right[trail_id]),
        mouse_name = 
          session[[session_id]]$mouse_name,
        session_id = session_id
      ) 
    
    # Append the trail data to the list
    trial_data_list[[trail_id]] <- trail_tibble
  }
  
  # Combine all trial data into a single tibble
  session_data <- bind_rows(trial_data_list)
  
  return(session_data)
}

session_1<- get_session_data(1)
session_1
```

```{r}
session_data_list <- list()

for (i in 1:18) {
  x <- get_session_data(i)
  session_data_list[[i]] <- x
}

# Combine all session data into a single tibble
all_session_data <- bind_rows(session_data_list)
```

```{r}
# Compute the average spike rate for each session
average_spike_rate <- all_session_data %>% 
  group_by(session_id, trail_id) %>% 
  summarize(spike_rate = sum(region_sum_spike) / sum(region_count))

# Print the average spike rate for each session
print(average_spike_rate)

average_spike_rate %>% group_by(session_id)%>%
  ggplot(mapping = aes(x = trail_id, y = spike_rate))+
  geom_line()+
  geom_smooth()+
  labs(x = "Trial", y = "Overall Neuron Spike Rate", color = "Session ID") +
  ggtitle("Change of Overall Neuron Spike Rate for Each Session") +
  facet_wrap(~session_id, ncol = 6)

average_spike_rate2 <- all_session_data %>%
  group_by(mouse_name, session_id, trail_id) %>%
  summarize(total_spike_count = (sum(region_sum_spike) / sum(region_count)))
average_spike_rate2

# Plot the change of overall neuron spike rate for each mouse over time
# ggplot(average_spike_rate2, aes(x = trail_id, y = total_spike_count)) +
  geom_line() +
  geom_smooth()+
  labs(x = "Trial", y = "Overall Neuron Spike Rate", color = "Brain Area") +
  ggtitle("Change of Overall Neuron Spike Rate for Each Mouse") +
  facet_wrap(~ mouse_name, ncol = 2)+
  theme_minimal()
```
As time goes on, neuron spike rate decreases slightly for each mice especially at the end.

```{r}
ggplot(all_session_data, aes(x = session_id, y = brain_area, color = mouse_name)) +
  geom_point() +
  labs(x = "Session ID", y = "Brain Area", color = "Mouse Name") +
  theme_minimal()
common_brain_area<- all_session_data%>% group_by(brain_area)%>% count()
common_brain_area %>% filter(n>=1000) %>% arrange(n)
```
```{r}
success_rate1<- all_session_data %>% group_by(session_id)%>% summarise(success_rate = (sum(feedback_type == 1)/n()))
success_rate1

success_rate2<- all_session_data %>% group_by(mouse_name)%>% summarise(success_rate = (sum(feedback_type == 1)/n()))
# success_rate2
```

```{r}
contrast_diff_dist<- all_session_data %>% group_by(contrast_diff)%>% summarise(n = n(), perc = n()/nrow(all_session_data), labels = paste0(round(perc * 100, 2), "%"))
contrast_diff_dist

success_rate <- all_session_data %>%
  group_by(mouse_name, contrast_diff) %>%
  summarise(success_rate = sum(feedback_type == 1)/n())

success_rate%>% spread(key = contrast_diff, value = success_rate)

# Print the resulting data frame
print(success_rate)

ggplot(success_rate, aes(x = contrast_diff, y = success_rate, color = mouse_name)) +
  geom_point() +
  geom_line()+
  labs(x = "Contrast Difference", y = "Success Rate", title = "Success Rate vs. Contrast Difference by Mouse")

# Perform two-way ANOVA
anova_result <- aov(success_rate ~ mouse_name + contrast_diff, data = success_rate)

# Summarize the ANOVA results
summary(anova_result)
```
```{r}
all_session_data2 <- all_session_data %>%
  group_by(session_id)%>%
  mutate(trial_bin = cut(trail_id, breaks = seq(0, max(trail_id), by = 25), include.lowest = TRUE, include.highest = TRUE))
success_rate_data <- all_session_data2 %>%
  group_by(session_id, trial_bin) %>%
  summarise(avg_success_rate = sum(feedback_type == 1)/n(), na.rm = TRUE)

ggplot(success_rate_data, aes(x = trial_bin, y = avg_success_rate)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Trial Bin", y = "Average Success Rate", fill = "Session") +
  ggtitle("Average Success Rate Over Every 25 Trials") +
  facet_wrap(~ session_id, ncol = 3)+
  theme_minimal()
```
```{r}
success_rate_data2 <- all_session_data2 %>%
  group_by(mouse_name, trial_bin) %>%
  summarise(avg_success_rate = sum(feedback_type == 1)/n(), na.rm = TRUE)

ggplot(success_rate_data2, aes(x = trial_bin, y = avg_success_rate)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Trial Bin", y = "Average Success Rate", fill = "Session") +
  ggtitle("Average Success Rate Over Every 25 Trials") +
  facet_wrap(~ mouse_name, ncol = 2)+
  theme_minimal()
```
```{r}
# Wrapping up the function:
average_spike_area<-function(i.t,this_session){
  spk.trial = this_session$spks[[i.t]]
  area= this_session$brain_area
  spk.count=apply(spk.trial,1,sum)
  spk.average.tapply=tapply(spk.count, area, mean)
  return(spk.average.tapply)
  }

# Test the function
average_spike_area(1,this_session = session[[6]])

```
```{r}
trial.summary.func<- function(i.s){
  trial_summary =matrix(nrow=meta$n_trials[i.s],ncol= meta$n_brain_area[i.s]+7)
  for(i.t in 1:meta$n_trials[i.s]){
    trial_summary[i.t,]=c(average_spike_area(i.t,this_session = session[[i.s]]),
                          session[[i.s]]$feedback_type[i.t],
                        session[[i.s]]$contrast_left[i.t],
                        session[[i.s]]$contrast_right[i.t],
                        abs(session[[i.s]]$contrast_left[i.t] - session[[i.s]]$contrast_right[i.t]), 
                        session[[i.s]]$mouse_name,
                        i.t, i.s)
}

colnames(trial_summary)=c(names(average_spike_area(i.t, this_session = session[[i.s]])), 'feedback', 'left contr.','right contr.', 'contrast_diff', 'mouse_name', 'trail_id', 'session_id')

# Turning it into a tibble
trial_summary <- as_tibble(trial_summary)
return(trial_summary)
}
```

```{r}
total_tibble = tibble()
for (i in 1:n.session){
  n.trial=meta$n_trials[i]
  n.area=meta$n_brain_area[i]
  trial.summary<- trial.summary.func(i)
  area.col=rainbow(n=n.area,alpha=0.7)
# In base R, I usually initiate a blank plot before drawing anything on it
  plot(x=1,y=0, col='white',xlim=c(0,n.trial),ylim=c(0.5,2.2), xlab="Trials",ylab="Average      spike counts", main=paste("Spikes per area in Session", i))


  for(i in 1:n.area){
    lines(y=trial.summary[[i]],x=trial.summary$trail_id,col=area.col[i],lty=2,lwd=1)
    lines(smooth.spline(trial.summary$trail_id, trial.summary[[i]]),col=area.col[i],lwd=3)
  }
    legend("topright", 
      legend = colnames(trial.summary)[1:n.area], 
      col = area.col, 
      lty = 1, 
      cex = 0.8
  )
  total_tibble = bind_rows(total_tibble, trial.summary)
}
```


```{r}

plot.trial<-function(i.t, area, area.col, this_session){
    
    spks=this_session$spks[[i.t]];
    n.neuron=dim(spks)[1]
    time.points=this_session$time[[i.t]]
    
    plot(0,0,xlim=c(min(time.points),max(time.points)),ylim=c(0,n.neuron+1),col='white', xlab='Time (s)',yaxt='n', ylab='Neuron', main=paste('Trial ',i.t, 'feedback', this_session$feedback_type[i.t] ),cex.lab=1.5)
    for(i in 1:n.neuron){
        i.a=which(area== this_session$brain_area[i]);
        col.this=area.col[i.a]
        
        ids.spike=which(spks[i,]>0) # find out when there are spikes 
        if( length(ids.spike)>0 ){
            points(x=time.points[ids.spike],y=rep(i, length(ids.spike) ),pch='.',cex=2, col=col.this)
        }
      
    }
    
legend("topright", 
  legend = area, 
  col = area.col, 
  pch = 16, 
  cex = 0.8
  )
  }
    
```

```{r, fig.width=8, fig.height=8}
i.s = 6
n.trial=meta$n_trials[i.s]
trial.summary6<- trial.summary.func(6)
for (i in 1:5){
  varname = names(trial.summary6)
  area=varname[1:(length(varname)-5)]
  plot.trial(i, area, area.col,session[[i.s]])
}

```

```{r}
varname=names(trial.summary.func(6));
area=varname[1:(length(varname)-5)]
par(mfrow=c(1,2))
plot.trial(1,area, area.col,session[[6]])
plot.trial(2,area, area.col,session[[6]])

par(mfrow=c(1,1))
```
```{r}
n.session=length(session)

n_success = 0
n_trial = 0
for(i in 1:n.session){
    tmp = session[[i]];
    n_trial = n_trial + length(tmp$feedback_type);
    n_success = n_success + sum(tmp$feedback_type == 1);
}
n_success/n_trial
test2=list()
for(i in 1:2){
  test2[[i]]=readRDS(paste('./Data/test',i,'.rds',sep=''))
  }
```
```{r}
session_data<- function(i.s){
  n_obs = length(session[[i.s]]$feedback_type)
  dat = tibble(
    feedback = as.factor(session[[i.s]]$feedback_type),
    decision = rep('name', n_obs),
    avg_spikes = rep(0, n_obs),
    session_id = i.s,
    trial_id = rep('id', n_obs),
    mouse_name = rep('name', n_obs),
    contrast_diff = rep('diff', n_obs),
    contrast_left = rep('left', n_obs),
    contrast_right = rep('right', n_obs)
  )
    for (i in 1:n_obs){
    # decision 
    if (session[[i.s]]$contrast_left[i] > session[[i.s]]$contrast_right[i]){
        dat$decision[i] = '1' 
    } else if (session[[i.s]]$contrast_left[i] < session[[i.s]]$contrast_right[i]){
        dat$decision[i] = '2' 
    } else if (session[[i.s]]$contrast_left[i] == session[[i.s]]$contrast_right[i] 
               & session[[i.s]]$contrast_left[i] == 0){
        dat$decision[i] = '3' 
    } else{
        dat$decision[i] = '4' 
    }
    dat$trial_id[i]=i
    dat$mouse_name[i] = session[[i.s]]$mouse_name
    dat$contrast_diff[i] = abs(session[[i.s]]$contrast_left[i]- session[[i.s]]$contrast_right[i])
    dat$contrast_left[i] = session[[i.s]]$contrast_left[i]
    dat$contrast_right[i] = session[[i.s]]$contrast_right[i]
    
    # avg_spks
    spks.trial = session[[i.s]]$spks[[i]]
    total.spikes = apply(spks.trial, 1, sum)
    dat$avg_spikes[i] = mean(total.spikes)
    }

dat$decision = as.factor(dat$decision)
return(dat)
}
test_data<- function(i.s){
  n_obs = length(test2[[i.s]]$feedback_type)
  dat = tibble(
    feedback = as.factor(test2[[i.s]]$feedback_type),
    decision = rep('name', n_obs),
    avg_spikes = rep(0, n_obs),
    session_id = i.s,
    trial_id = rep('id', n_obs),
    mouse_name = rep('name', n_obs),
    contrast_diff = rep('diff', n_obs),
    contrast_left = rep('left', n_obs),
    contrast_right = rep('right', n_obs)
  )
    for (i in 1:n_obs){
    # decision 
    if (test2[[i.s]]$contrast_left[i] > test2[[i.s]]$contrast_right[i]){
        dat$decision[i] = '1' 
    } else if (test2[[i.s]]$contrast_left[i] < test2[[i.s]]$contrast_right[i]){
        dat$decision[i] = '2' 
    } else if (test2[[i.s]]$contrast_left[i] == test2[[i.s]]$contrast_right[i] 
               & test2[[i.s]]$contrast_left[i] == 0){
        dat$decision[i] = '3' 
    } else{
        dat$decision[i] = '4' 
    }
    dat$trial_id[i]=i
    dat$mouse_name[i] = test2[[i.s]]$mouse_name
    dat$contrast_diff[i] = abs(test2[[i.s]]$contrast_left[i]- test2[[i.s]]$contrast_right[i])
    dat$contrast_left[i] = test2[[i.s]]$contrast_left[i]
    dat$contrast_right[i] = test2[[i.s]]$contrast_right[i]
    
    # avg_spks
    spks.trial = test2[[i.s]]$spks[[i]]
    total.spikes = apply(spks.trial, 1, sum)
    dat$avg_spikes[i] = mean(total.spikes)
    }

dat$decision = as.factor(dat$decision)
return(dat)
}

data_tibble = tibble()
for (i in 1:18){
  session.data = session_data(i)
  data_tibble = bind_rows(data_tibble, session.data)
}

test_tibble = tibble()
for (i in 1:2){
  test.data = test_data(i)
  test_tibble = bind_rows(test_tibble, test.data)
}
```

```{r}
set.seed(101)
sample <- sample.int(n = nrow(data_tibble), size = floor(.8 * nrow(data_tibble)), replace = F)
train <- data_tibble[sample, ]
test  <- data_tibble[-sample, ]
fit1 <- glm(feedback~avg_spikes+contrast_diff, data = train, family="binomial")
summary(fit1)

```

```{r}
#log model 1
pred1 <- predict(fit1, newdata = test, type = 'response')
prediction1 <- ifelse((pred1 >= 0.5), 1, -1)
actual<- test$feedback
mean(prediction1 != actual)
accuracy1<- sum(prediction1 == actual)/length(actual)
accuracy1
```

```{r}
#bias model
prediction0 = factor(rep('1', nrow(test)), levels = c('1', '-1'))
mean(prediction0 != test$feedback)
cm0 <- confusionMatrix(prediction0, test$feedback, dnn = c("Prediction", "Reference"))

plt0 <- as.data.frame(cm0$table)

ggplot(plt0, aes(Reference, Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("-1","1")) +
        scale_y_discrete(labels=c("-1","1"))
```
```{r}
#knn model 2
model2 <- train(feedback ~ avg_spikes+contrast_diff, 
               data = train, 
               method = "knn", 
               trControl = trainControl(method = "cv"), 
               tuneLength = 10)  

# Predict on test data
pred2 <- predict(model2, newdata = test)

# Evaluate model performance
confusion_matrix2 <- confusionMatrix(pred2, test$feedback)
accuracy2 <- confusion_matrix2$overall["Accuracy"]
precision2 <- confusion_matrix2$byClass["Precision"]
recall2 <- confusion_matrix2$byClass["Recall"]
f1_score2 <- confusion_matrix2$byClass["F1"]

# Display results
print(confusion_matrix2)
cat("\nAccuracy:", accuracy2)
cat("\nPrecision:", precision2)
cat("\nRecall:", recall2)
cat("\nF1 Score:", f1_score2)
```
```{r}
#xgboost model
library(xgboost)

set.seed(101) # for reproducibility
data_tibble1 = as.matrix(data_tibble)
intrain<-createDataPartition(y=data_tibble1[ , 1], p=0.8, list=FALSE) 
train1<-data_tibble1[intrain, ]
test1<-data_tibble1[-intrain, ]

train_X <- as.matrix(train1[, c(2, 3, 7)])
train_label <- as.numeric(train1[, 1])

test_X <- as.matrix(test1[, c(2, 3, 7)])
test_label <- as.numeric(test1[, 1])

mode(train_X) <- 'double'  # to numeric i.e double precision
mode(test_X) <- 'double'

train_label_binary <- ifelse(train_label > 0, 1, 0)
test_label_binary <- ifelse(test_label > 0, 1, 0)

# Train xgboost model with corrected labels
xgb_model <- xgboost(data = train_X, label = train_label_binary, max.depth = 2, eta = 1,
                     nround = 10, objective = "binary:logistic")

# Make predictions on the test set
pred3 <- predict(xgb_model, newdata = test_X)
predicted_labels <- as.numeric(ifelse(pred3 > 0.5, 1, 0))

# Calculate accuracy
accuracy <- mean(predicted_labels == test_label_binary)
accuracy
cm <- confusionMatrix(factor(predicted_labels), factor(test_label_binary))
cm
```
```{r}
library(ROCR)
pr1 = prediction(pred1, test$feedback)
prf <- performance(pr1, measure = "tpr", x.measure = "fpr")
auc <- performance(pr1, measure = "auc")
auc <- auc@y.values[[1]]

# Model 2
pred21 = as.numeric(pred2)
mode(pred21)<- 'double'
pred22 = list(pred21)
pr2 = prediction(pred22, test$feedback)
prf2 <- performance(pr2, measure = "tpr", x.measure = "fpr")
auc2 <- performance(pr2, measure = "auc")
auc2 <- auc2@y.values[[1]]

# Model 3
pr3 = prediction(pred3, test_label_binary)
prf3 <- performance(pr3, measure = "tpr", x.measure = "fpr")
auc3 <- performance(pr3, measure = "auc")
auc3 <- auc3@y.values[[1]]

# Bias Guess
pred0 = pred1 * 0 + 1
pr = prediction(pred0, test$feedback)
prf0 <- performance(pr, measure = "tpr", x.measure = "fpr")
auc0 <- performance(pr, measure = "auc")
auc0 <- auc0@y.values[[1]]

plot(prf, ,col = 'red', main = 'ROC curve')
plot(prf2, add = TRUE, col = 'blue')
plot(prf0, add = TRUE, col = 'green')
plot(prf3, add = TRUE, col = 'brown')
legend("bottomright", legend=c("Model 1", "Model 2", 'Model 3', "Bias Guess"), col=c("red", "blue", 'brown', 'green'), lty=1:1, 
       cex=0.8)
```

## Test Data
```{r}
#bias model on test data
prediction0.2 = factor(rep('1', nrow(test_tibble)), levels = c('1', '-1'))
mean(prediction0.2 != test_tibble$feedback)
cm0.2 <- confusionMatrix(prediction0.2, test_tibble$feedback, dnn = c("Prediction", "Reference"))

plt0.2 <- as.data.frame(cm0.2$table)

ggplot(plt0.2, aes(Reference, Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("-1","1")) +
        scale_y_discrete(labels=c("-1","1"))
```

```{r}
#log model on test data
fit1.2 <- glm(feedback~avg_spikes+contrast_diff, data = data_tibble, family="binomial")
pred1.2<-predict(fit1.2, newdata = test_tibble, type = 'response')
prediction1.2 <- ifelse((pred1.2 >= 0.5), 1, -1)
actual<- test_tibble$feedback
accuracy1.2<- sum(prediction1.2 == actual)/length(actual)
accuracy1.2
```

```{r}
#knn model on test data
model2.2 <- train(feedback ~ avg_spikes+contrast_diff, 
               data = data_tibble, 
               method = "knn", 
               trControl = trainControl(method = "cv"), 
               tuneLength = 10) 

# Predict on test data
pred2.2 <- predict(model2.2, newdata = test_tibble)

# Evaluate model performance
confusion_matrix2.2 <- confusionMatrix(pred2.2, test_tibble$feedback)
accuracy2.2 <- confusion_matrix2.2$overall["Accuracy"]
precision2.2 <- confusion_matrix2.2$byClass["Precision"]
recall2.2 <- confusion_matrix2.2$byClass["Recall"]
f1_score2.2 <- confusion_matrix2.2$byClass["F1"]

# Display results
print(confusion_matrix2.2)
cat("\nAccuracy:", accuracy2.2)
cat("\nPrecision:", precision2.2)
cat("\nRecall:", recall2.2)
cat("\nF1 Score:", f1_score2.2)
```

```{r}
#xgboost model on test data
train3.2<-data_tibble1
test3.2 = as.matrix(test_tibble)

train_X2 <- as.matrix(train3.2[, c(2, 3, 7)])
train_label2 <- as.numeric(train3.2[, 1])

test_X2 <- as.matrix(test3.2[, c(2, 3, 7)])
test_label2 <- as.numeric(test3.2[, 1])

mode(train_X2) <- 'double'  # to numeric i.e double precision
mode(test_X2) <- 'double'

train_label_binary2 <- ifelse(train_label2 > 0, 1, 0)
test_label_binary2 <- ifelse(test_label2 > 0, 1, 0)

# Train xgboost model with corrected labels
xgb_model2 <- xgboost(data = train_X2, label = train_label_binary2, max.depth = 2, eta = 1,
                     nround = 10, objective = "binary:logistic")

# Make predictions on the test set
pred3.2 <- predict(xgb_model2, newdata = test_X2)
predicted_labels2 <- as.numeric(ifelse(pred3.2 > 0.5, 1, 0))

# Calculate accuracy
accuracy2 <- mean(predicted_labels2 == test_label2)
accuracy2
```
